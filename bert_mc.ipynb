{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/opt/developer/wp/wzcq/roberta_wwm/bert_config.json'\n",
    "checkpoint_path = '/opt/developer/wp/wzcq/roberta_wwm/bert_model.ckpt'\n",
    "dict_path = '/opt/developer/wp/wzcq/roberta_wwm/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad_examples(input_file):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]: \n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                train_data = {}\n",
    "                train_data[\"context\"] = paragraph_text\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                answer = qa[\"answers\"][0]\n",
    "                orig_answer_text = answer[\"text\"]\n",
    "                train_data[\"quary\"] = question_text\n",
    "                train_data[\"ans\"] =  orig_answer_text \n",
    "                examples.append(train_data)\n",
    "    return examples           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(token_file):\n",
    "    with open(token_file,\"r\") as f:\n",
    "        token_list = f.readlines()\n",
    "        token_dict = {word.strip():id_ for id_,word in enumerate(token_list)}\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "token_dict = get_token_dict(dict_path)\n",
    "tokenizer = OurTokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_ans_doc(ans,doc):\n",
    "    doc = re.sub(\"\\s+\",\"\",doc)\n",
    "    ans = re.sub(\"\\s+\",\"\",ans)\n",
    "    sub_doc = doc.split(\"。\")\n",
    "    for i in range(len(sub_doc)):\n",
    "        try:\n",
    "            if ans in  sub_doc[i]+\"。\":\n",
    "                return ans, sub_doc[i-1]+ \"。\" + sub_doc[i] + \"。\" + sub_doc[i+1]\n",
    "        except:\n",
    "            return ans, sub_doc[i-1]+ \"。\" + sub_doc[i] + \"。\"\n",
    "    else:\n",
    "#         print (\"no ans\")\n",
    "        return None,None\n",
    "        \n",
    "        \n",
    "        \n",
    "def get_short_data(data):\n",
    "    new_data_list = []\n",
    "    for d in data:\n",
    "#         print(d)\n",
    "        new_ans,new_context = extract_ans_doc(d[\"ans\"],d[\"context\"])\n",
    "        if new_context is not None and new_ans is not None:\n",
    "            d[\"context\"] = new_context\n",
    "            d[\"ans\"] = new_ans\n",
    "            new_data_list.append(d)    \n",
    "        else:\n",
    "            pass\n",
    "    return new_data_list    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_squad_examples(\"squad-style-data/cmrc2018_train.json\")\n",
    "data_new = get_short_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9906"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业',\n",
       " 'quary': '1990年，范廷颂担任什么职务？',\n",
       " 'ans': '1990年被擢升为天主教河内总教区宗座署理'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(i[\"context\"]) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0,max_len=512):\n",
    "    ML = max_len\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x[:max_len] for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "def list_find(list1, list2):\n",
    "    \"\"\"在list1中寻找子串list2，如果找到，返回第一个下标；\n",
    "    如果找不到，返回-1。\n",
    "    \"\"\"\n",
    "    n_list2 = len(list2)\n",
    "    for i in range(len(list1)):\n",
    "        if list1[i: i+n_list2] == list2:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=16):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, S1, S2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text, c = d[\"context\"], d[\"quary\"]\n",
    "                text = u'___%s___%s' % (c, text)\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                e = d[\"ans\"]\n",
    "                e_tokens = tokenizer.tokenize(e)[1:-1]\n",
    "                s1, s2 = np.zeros(len(tokens)), np.zeros(len(tokens))\n",
    "                start = list_find(tokens, e_tokens)\n",
    "                if start != -1:\n",
    "                    end = start + len(e_tokens) - 1\n",
    "                    s1[start] = 1\n",
    "                    s2[end] = 1\n",
    "                    x1, x2 = tokenizer.encode(first=text)\n",
    "                    X1.append(x1)\n",
    "                    X2.append(x2)\n",
    "                    S1.append(s1)\n",
    "                    S2.append(s2)\n",
    "                    if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                        X1 = seq_padding(X1)\n",
    "                        X2 = seq_padding(X2)\n",
    "                        S1 = seq_padding(S1)\n",
    "                        S2 = seq_padding(S2)\n",
    "                        yield [X1, X2, S1, S2], None\n",
    "                        X1, X2, S1, S2 = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = data_generator(data_new,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, None, 768)    101677056   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      768         model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 1)      768         model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None)         0           dense_1[0][0]                    \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None)         0           dense_2[0][0]                    \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 101,678,592\n",
      "Trainable params: 101,678,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/developer/wp/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output lambda_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to lambda_2.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n",
      "/opt/developer/wp/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output lambda_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to lambda_3.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "x1_in = Input(shape=(None,)) # 待识别句子输入\n",
    "x2_in = Input(shape=(None,)) # 待识别句子输入\n",
    "s1_in = Input(shape=(None,)) # 实体左边界（标签）\n",
    "s2_in = Input(shape=(None,)) # 实体右边界（标签）\n",
    "\n",
    "x1, x2, s1, s2 = x1_in, x2_in, s1_in, s2_in\n",
    "x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x1)\n",
    "\n",
    "x = bert_model([x1, x2])\n",
    "ps1 = Dense(1, use_bias=False)(x)\n",
    "\n",
    "\n",
    "###[[0.1],[0.2],[0.3]..] -> [0.1,0.2,0.3,...] \n",
    "###[0.1,0.2,0.3,...] - [0,0,0,0,1,1,1,1]*1e10 \n",
    "ps1 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10)([ps1, x_mask])\n",
    "# ps1 = Lambda(lambda x: x[0]*x[1])([ps1, x_mask])\n",
    "\n",
    "\n",
    "ps2 = Dense(1, use_bias=False)(x)\n",
    "ps2 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10)([ps2, x_mask])\n",
    "# ps2 = Lambda(lambda x:x[0]*x[1])([ps2, x_mask])\n",
    "\n",
    "model = Model([x1_in, x2_in], [ps1, ps2])\n",
    "\n",
    "\n",
    "train_model = Model([x1_in, x2_in, s1_in, s2_in], [ps1, ps2])\n",
    "###[0,0,1,0,0]  [0.1,0.1,0.8,-1e10,-1e10,-1e10]\n",
    "loss1 = K.mean(K.categorical_crossentropy(s1_in, ps1, from_logits=True)) \n",
    "### K.cumsum(s1, 1) = [0,0,0,0,1,1,1,1]\n",
    "### ps2 - [1,1,1,1,0,0,0,0]* 1e10 降低ps2在ps1 之前的概率\n",
    "ps2 -= (1 - K.cumsum(s1, 1)) * 1e10\n",
    "loss2 = K.mean(K.categorical_crossentropy(s2_in, ps2, from_logits=True))\n",
    "loss = loss1 + loss2\n",
    "\n",
    "train_model.add_loss(loss)\n",
    "train_model.compile(optimizer=Adam(learning_rate=0.00001))\n",
    "train_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2477/2477 [==============================] - 830s 335ms/step - loss: 0.2254\n",
      "Epoch 2/3\n",
      "2477/2477 [==============================] - 830s 335ms/step - loss: 0.1667\n",
      "Epoch 3/3\n",
      "2477/2477 [==============================] - 830s 335ms/step - loss: 0.1447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2eb42a3470>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.fit_generator(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_D),\n",
    "                          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mc_weights_new.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    x = np.exp(x)\n",
    "    return x / np.sum(x)\n",
    "\n",
    "def extract_entity(text_in, c_in):\n",
    "#     print(text_in)\n",
    "    print(c_in)\n",
    "    text_in = u'___%s___%s' % (c_in, text_in)\n",
    "    text_in = text_in[:510]\n",
    "    _tokens = tokenizer.tokenize(text_in)\n",
    "    _x1, _x2 = tokenizer.encode(first=text_in)\n",
    "    _x1, _x2 = np.array([_x1]), np.array([_x2])\n",
    "    _ps1, _ps2  = model.predict([_x1, _x2])\n",
    "    _ps1, _ps2 = softmax(_ps1[0]), softmax(_ps2[0])\n",
    "    start = _ps1.argmax()\n",
    "    print(start)\n",
    "    end = _ps2[start:].argmax() + start\n",
    "    print(end)\n",
    "    a = text_in[start-1: end]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = read_squad_examples(\"squad-style-data/cmrc2018_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "广三铁路在哪年建成？\n",
      "415\n",
      "419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1903年'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entity(data_test[9][\"context\"],data_test[9][\"quary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恺英的主要业务是什么？\n",
      "379\n",
      "475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n未来，恺英网络将持续深耕游戏业务，围绕“游戏+内容+互联网高科技”的战略部署，强化产业链上下游实力，拓展海外市场，\\n继续为用户提供优质内容服务和深度游戏娱乐体验，打造卓越的互联网游戏上市公司。'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entity('''恺英网络股份有限公司（SZ.002517）是国内领先的互动娱乐综合服务商。\n",
    "主要业务涵盖页游与手游等游戏业务的研发、运营及发行，网页游戏平台、移动应用分发平台的运营，\n",
    "以及VR、大数据智能处理等互联网高科技业务投资。恺英网络始终秉持“专注品质、用心服务”的理念，\n",
    "坚持“研发+发行双轮驱动”，并在全球范围内搜寻和引进优质IP，通过游戏产品（移动游戏、网页游戏、H5游戏）\n",
    "和发行平台（XY页游平台、XY助手、XY游、MG游戏）进行横向延伸，通过电竞、动画、漫画、影视剧等泛娱乐内容进行纵向布局，\n",
    "全力为用户打造优质感官体验。旗下上海恺英、浙江盛和及浙江九翎先后开发并运营了《摩天大楼》、《蜀山传奇》、《全民奇迹MU》、\n",
    "《王者传奇》、《蓝月传奇》、《敢达争锋对决》、《战舰世界闪击战》、H5游戏《传奇来了》等多款热门游戏。\n",
    "未来，恺英网络将持续深耕游戏业务，围绕“游戏+内容+互联网高科技”的战略部署，强化产业链上下游实力，拓展海外市场，\n",
    "继续为用户提供优质内容服务和深度游戏娱乐体验，打造卓越的互联网游戏上市公司。''',\"恺英的主要业务是什么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
